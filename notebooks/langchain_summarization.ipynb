{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanchain : Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "import arxiv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import umap\n",
    "import yaml\n",
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from rich import print\n",
    "from sklearn.cluster import DBSCAN\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BigBirdPegasusForConditionalGeneration,\n",
    "    PegasusTokenizerFast,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda:0\":\n",
    "    print(torch.cuda.get_device_properties(device))\n",
    "else:\n",
    "    print(f\"No cuda device found; running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/local/hf_secrets.yml\", \"r\") as f:\n",
    "    hf_secrets = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run search query on arxiv\n",
    "search = arxiv.Search(\n",
    "    query=\"ti:LLM OR (ti:LARGE AND ti:LANGUAGE AND ti:MODEL)\",\n",
    "    max_results=100,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "\n",
    "# display some results\n",
    "for result in list(search.results())[:2]:\n",
    "    print(f\"--- {result.title} [{result.published}] ---\")\n",
    "    print(result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting models to try:\n",
    "* [google/bigbird-pegasus-large-arxiv](https://huggingface.co/google/bigbird-pegasus-large-arxiv)\n",
    "* [allenai/led-large-16384-arxiv](https://huggingface.co/allenai/led-large-16384-arxiv)\n",
    "* [google/pegasus-arxiv](https://huggingface.co/google/pegasus-arxiv)\n",
    "* [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) (very interesting summarization results from that one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting links for embeddings & langchain:\n",
    "* [HuggingFaceBgeEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceBgeEmbeddings.html?highlight=device)\n",
    "* [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html?highlight=device)\n",
    "* [HuggingFaceInstructEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceInstructEmbeddings.html?highlight=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"google/bigbird-pegasus-large-arxiv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get abstract\n",
    "result = next(search.results())\n",
    "summary = result.summary\n",
    "\n",
    "# feed-forward pass\n",
    "inputs = tokenizer([summary], max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model(**inputs, return_dict=True)\n",
    "\n",
    "# Generate Summary\n",
    "# summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=15)\n",
    "# model_summary = tokenizer.batch_decode(summary_ids,\n",
    "# skip_special_tokens=True,\n",
    "# clean_up_tokenization_spaces=False)[0]\n",
    "# print(f\"model_summary: '{model_summary}'\")\n",
    "\n",
    "# Retrieve embedding\n",
    "cls_token_embed = outputs.encoder_last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_pegasus(\n",
    "    input_text: Union[str, List[str]],\n",
    "    model: BigBirdPegasusForConditionalGeneration,\n",
    "    tokenizer: PegasusTokenizerFast,\n",
    "    batch_size: Optional[int] = None,\n",
    "    device: str = \"cpu\",\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    model = model.to(device)\n",
    "\n",
    "    dim = model.config.hidden_size\n",
    "\n",
    "    if isinstance(input_text, str):\n",
    "        inputs = tokenizer(\n",
    "            [input_text],\n",
    "            max_length=4096,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "        )\n",
    "        outputs = model(**inputs.to(device), return_dict=True)\n",
    "        cls_token_embeds = outputs.encoder_last_hidden_state[:, 0, :]\n",
    "\n",
    "        del outputs\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    else:\n",
    "        if batch_size is not None:\n",
    "            indices = list(range(0, len(input_text), batch_size)) + [None]\n",
    "            cls_token_embeds = torch.empty((0, dim), device=device)\n",
    "\n",
    "            for i in range(len(indices) - 1):\n",
    "                input_text_i = input_text[indices[i] : indices[i + 1]]\n",
    "                inputs_i = tokenizer(\n",
    "                    input_text_i,\n",
    "                    max_length=4096,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                )\n",
    "                outputs_i = model(**inputs_i.to(device), return_dict=True)\n",
    "                cls_token_embeds_i = outputs_i.encoder_last_hidden_state[:, 0, :]\n",
    "                cls_token_embeds = torch.cat((cls_token_embeds, cls_token_embeds_i), dim=0)\n",
    "\n",
    "                del outputs_i\n",
    "                del inputs_i\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        else:\n",
    "            inputs = tokenizer(input_text, max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "            outputs = model(**inputs.to(device), return_dict=True)\n",
    "            cls_token_embeds = outputs.encoder_last_hidden_state[:, 0, :]\n",
    "\n",
    "            del outputs\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return cls_token_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve abstracts & metadata\n",
    "abstracts = [result.summary for result in search.results()]\n",
    "titles = [result.title for result in search.results()]\n",
    "\n",
    "# embed abstracts\n",
    "with torch.no_grad():\n",
    "    embeddings = embed_pegasus(\n",
    "        input_text=abstracts,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=20,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "# convert to numpy\n",
    "if device == \"cpu\":\n",
    "    embeddings = embeddings.numpy()\n",
    "else:\n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "\n",
    "# reduce dimension\n",
    "reducer = umap.UMAP(n_components=2, n_neighbors=5, metric=\"cosine\", min_dist=0.1)\n",
    "umap_coords = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results in a dataframe\n",
    "df = pd.DataFrame(data=umap_coords, columns=[f\"umap_{i}\" for i in range(umap_coords.shape[1])])\n",
    "df[\"title\"] = titles\n",
    "\n",
    "# display\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz\n",
    "fig = px.scatter(df, x=\"umap_0\", y=\"umap_1\", hover_data=\"title\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering with (H)DBSCAN\n",
    "\n",
    "clusterer = DBSCAN(eps=0.8)\n",
    "clusterer.fit(df[[col for col in df.columns if \"umap\" in col]])\n",
    "df[\"cluster\"] = clusterer.labels_\n",
    "\n",
    "# viz\n",
    "fig = px.scatter(df, x=\"umap_0\", y=\"umap_1\", hover_data=\"title\", color=\"cluster\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize clusters of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting links:\n",
    "* [https://python.langchain.com/docs/integrations/retrievers/arxiv](https://python.langchain.com/docs/integrations/retrievers/arxiv)\n",
    "* [https://python.langchain.com/docs/additional_resources/tutorials](https://python.langchain.com/docs/additional_resources/tutorials)\n",
    "* [https://python.langchain.com/docs/use_cases/summarization](https://python.langchain.com/docs/use_cases/summarization)\n",
    "* [https://huggingface.co/facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)\n",
    "* [https://python.langchain.com/docs/integrations/llms/openllm](https://python.langchain.com/docs/integrations/llms/openllm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "repo_id = \"facebook/bart-large-cnn\"\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.15,\n",
    "        \"max_length\": 100,\n",
    "        # \"device\": device,\n",
    "    },\n",
    "    huggingfacehub_api_token=hf_secrets[\"hf_hub_token\"],\n",
    ")\n",
    "\n",
    "\n",
    "# question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "# template = \"\"\"\n",
    "# Question: {question}\n",
    "\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build docs\n",
    "\n",
    "cluster_idx = 0\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=abstracts[i],\n",
    "        metadata={\"title\": titles[i]},\n",
    "    )\n",
    "    for i in range(len(titles))\n",
    "    if df.loc[df[\"title\"] == titles[i], \"cluster\"].values[0] == cluster_idx\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_reduce_chain.run(split_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Refine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
