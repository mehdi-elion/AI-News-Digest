{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Docker-Compose Setup\n",
    "\n",
    "In this notebook, we'll test the docker-compose setup. First, open a terminal, \n",
    "`cd` into the root directory of this repo and run the following command (c.f.\n",
    "[README.md](../README.md) for more details):\n",
    "```bash\n",
    "# .env must be available at the root as shown in .env.example\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "You should see a series of logs indicating a successful run of your services.\n",
    "If so, run the following cells to test your services.\n",
    "\n",
    "This notebook is designed to be minimalist and as standalone as possible, so \n",
    "don't hesitate to re-use it and adjust it to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libs\n",
    "import arxiv\n",
    "from rich import print\n",
    "import qdrant_client\n",
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "QDRANT_URL = \"http://localhost\"\n",
    "QDRANT_PORT = 6335\n",
    "EMBED_API_URL = \"http://localhost:8081\"\n",
    "GEN_API_URL = \"http://localhost:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "envvars = dotenv.dotenv_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Microservices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melion/miniconda3/envs/ai_news/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Embeddings from <span style=\"color: #008000; text-decoration-color: #008000\">'BAAI/bge-small-en-v1.5'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Embeddings from \u001b[32m'BAAI/bge-small-en-v1.5'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-0.0076529826, -0.02522551, -0.024398882]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to embed_api\n",
    "embed_model = HuggingFaceHubEmbeddings(model=EMBED_API_URL)\n",
    "\n",
    "# test it !\n",
    "text = \"What is deep learning?\"\n",
    "query_result = embed_model.embed_query(text)\n",
    "print(f\"Embeddings from '{envvars['EMBED_MODEL_ID']}'\")\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generation from <span style=\"color: #008000; text-decoration-color: #008000\">'google/gemma-2b-it'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generation from \u001b[32m'google/gemma-2b-it'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "Machines, vast and grand, with minds of steel,\n",
       "A symphony of algorithms, we can't fail.\n",
       "Learning from data, patterns emerge,\n",
       "Discovering truths, with each passing year.\n",
       "\n",
       "From medical scans to marketing trends,\n",
       "They weave insights, the tapestry extends.\n",
       "With each iteration, they refine and adapt,\n",
       "Unleashing potential, a digital bath.\n",
       "\n",
       "But machines, though potent, hold a human face,\n",
       "With biases and limitations, a note.\n",
       "We must ensure fairness, a moral guide,\n",
       "To prevent harm, to protect our tide.\n",
       "\n",
       "So let's embrace this revolution in our time,\n",
       "Where humans and machines, a harmonious chime.\n",
       "With collaboration and understanding, we'll soar,\n",
       "To build a future, where possibilities soar.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "Machines, vast and grand, with minds of steel,\n",
       "A symphony of algorithms, we can't fail.\n",
       "Learning from data, patterns emerge,\n",
       "Discovering truths, with each passing year.\n",
       "\n",
       "From medical scans to marketing trends,\n",
       "They weave insights, the tapestry extends.\n",
       "With each iteration, they refine and adapt,\n",
       "Unleashing potential, a digital bath.\n",
       "\n",
       "But machines, though potent, hold a human face,\n",
       "With biases and limitations, a note.\n",
       "We must ensure fairness, a moral guide,\n",
       "To prevent harm, to protect our tide.\n",
       "\n",
       "So let's embrace this revolution in our time,\n",
       "Where humans and machines, a harmonious chime.\n",
       "With collaboration and understanding, we'll soar,\n",
       "To build a future, where possibilities soar.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# connect to gen_api\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=GEN_API_URL,\n",
    "    model_name=envvars[\"GEN_MODEL_ID\"],\n",
    "    # model_kwargs={\"stop\": [\".\"]},\n",
    ")\n",
    "\n",
    "# test it !\n",
    "answer = llm.predict(\"Write me a poem about Machine Learning.\")\n",
    "print(f\"Generation from '{envvars['GEN_MODEL_ID']}'\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Deleted collection <span style=\"color: #008000; text-decoration-color: #008000\">'arxiv_documents'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Deleted collection \u001b[32m'arxiv_documents'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# connect to qdrant\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    port=6333\n",
    "    # api_key=\"<qdrant-api-key>\", # For Qdrant Cloud, None for local instance\n",
    ")\n",
    "\n",
    "# test it\n",
    "collections = client.get_collections()\n",
    "for c in collections.collections:\n",
    "    client.delete_collection(c.name)\n",
    "    print(f\"Deleted collection '{c.name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25981/2060595641.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in list(search.results())[:2]:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- <span style=\"color: #008000; text-decoration-color: #008000\">'Deep Learning'</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:20:34</span>+<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">00:00</span><span style=\"font-weight: bold\">]</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- \u001b[32m'Deep Learning'\u001b[0m \u001b[1m[\u001b[0m\u001b[1;36m2018\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m20\u001b[0m \u001b[1;92m18:20:34\u001b[0m+\u001b[1;92m00:00\u001b[0m\u001b[1m]\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Deep learning <span style=\"font-weight: bold\">(</span>DL<span style=\"font-weight: bold\">)</span> is a high dimensional data reduction technique for\n",
       "constructing high-dimensional predictors in input-output models. DL is a form\n",
       "of machine learning that uses hierarchical layers of latent features. In this\n",
       "article, we review the state-of-the-art of deep learning from a modeling and\n",
       "algorithmic perspective. We provide a list of successful areas of applications\n",
       "in Artificial Intelligence <span style=\"font-weight: bold\">(</span>AI<span style=\"font-weight: bold\">)</span>, Image Processing, Robotics and Automation.\n",
       "Deep learning is predictive in its nature rather then inferential and can be\n",
       "viewed as a black-box methodology for high-dimensional function estimation. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Deep learning \u001b[1m(\u001b[0mDL\u001b[1m)\u001b[0m is a high dimensional data reduction technique for\n",
       "constructing high-dimensional predictors in input-output models. DL is a form\n",
       "of machine learning that uses hierarchical layers of latent features. In this\n",
       "article, we review the state-of-the-art of deep learning from a modeling and\n",
       "algorithmic perspective. We provide a list of successful areas of applications\n",
       "in Artificial Intelligence \u001b[1m(\u001b[0mAI\u001b[1m)\u001b[0m, Image Processing, Robotics and Automation.\n",
       "Deep learning is predictive in its nature rather then inferential and can be\n",
       "viewed as a black-box methodology for high-dimensional function estimation. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- <span style=\"color: #008000; text-decoration-color: #008000\">'Deep, Deep Learning with BART'</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:23:41</span>+<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">00:00</span><span style=\"font-weight: bold\">]</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- \u001b[32m'Deep, Deep Learning with BART'\u001b[0m \u001b[1m[\u001b[0m\u001b[1;36m2022\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m28\u001b[0m \u001b[1;92m18:23:41\u001b[0m+\u001b[1;92m00:00\u001b[0m\u001b[1m]\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Purpose: To develop a deep-learning-based image reconstruction framework for\n",
       "reproducible research in MRI.\n",
       "  Methods: The BART toolbox offers a rich set of implementations of calibration\n",
       "and reconstruction algorithms for parallel imaging and compressed sensing. In\n",
       "this work, BART was extended by a non-linear operator framework that provides\n",
       "automatic differentiation to allow computation of gradients. Existing\n",
       "MRI-specific operators of BART, such as the non-uniform fast Fourier transform,\n",
       "are directly integrated into this framework and are complemented by common\n",
       "building blocks used in neural networks. To evaluate the use of the framework\n",
       "for advanced deep-learning-based reconstruction, two state-of-the-art unrolled\n",
       "reconstruction networks, namely the Variational Network <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span> and MoDL <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>, were\n",
       "implemented.\n",
       "  Results: State-of-the-art deep image-reconstruction networks can be\n",
       "constructed and trained using BART's gradient based optimization algorithms.\n",
       "The BART implementation achieves a similar performance in terms of training\n",
       "time and reconstruction quality compared to the original implementations based\n",
       "on TensorFlow.\n",
       "  Conclusion: By integrating non-linear operators and neural networks into\n",
       "BART, we provide a general framework for deep-learning-based reconstruction in\n",
       "MRI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Purpose: To develop a deep-learning-based image reconstruction framework for\n",
       "reproducible research in MRI.\n",
       "  Methods: The BART toolbox offers a rich set of implementations of calibration\n",
       "and reconstruction algorithms for parallel imaging and compressed sensing. In\n",
       "this work, BART was extended by a non-linear operator framework that provides\n",
       "automatic differentiation to allow computation of gradients. Existing\n",
       "MRI-specific operators of BART, such as the non-uniform fast Fourier transform,\n",
       "are directly integrated into this framework and are complemented by common\n",
       "building blocks used in neural networks. To evaluate the use of the framework\n",
       "for advanced deep-learning-based reconstruction, two state-of-the-art unrolled\n",
       "reconstruction networks, namely the Variational Network \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m and MoDL \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m, were\n",
       "implemented.\n",
       "  Results: State-of-the-art deep image-reconstruction networks can be\n",
       "constructed and trained using BART's gradient based optimization algorithms.\n",
       "The BART implementation achieves a similar performance in terms of training\n",
       "time and reconstruction quality compared to the original implementations based\n",
       "on TensorFlow.\n",
       "  Conclusion: By integrating non-linear operators and neural networks into\n",
       "BART, we provide a general framework for deep-learning-based reconstruction in\n",
       "MRI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run a query through Arxiv API\n",
    "search = arxiv.Search(\n",
    "    query=\"ti:deep AND ti:learning\",\n",
    "    max_results=300,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    ")\n",
    "\n",
    "# display some results\n",
    "for result in list(search.results())[:2]:\n",
    "    print(f\"--- '{result.title}' [{result.published}] ---\")\n",
    "    print(result.summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25981/2918398908.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for i, result in enumerate(search.results())\n"
     ]
    }
   ],
   "source": [
    "# create Langchain documents from query results\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=result.summary,\n",
    "        metadata={\n",
    "            \"title\": result.title,\n",
    "            \"publish_date\": result.published,\n",
    "        },\n",
    "    )\n",
    "    for i, result in enumerate(search.results())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sore results in qdrant\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs, \n",
    "    embed_model, \n",
    "    url=QDRANT_URL, \n",
    "    # prefer_grpc=True, \n",
    "    collection_name=\"arxiv_documents\",\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA using Qdrant & vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['context', 'question'], \n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['context', 'question'], \n",
    "                template=\"You are an assistant for question-answering tasks. \" \\\n",
    "                    \"Use the following pieces of retrieved context to answer \" \\\n",
    "                    \"the question. If you don't know the answer, just say \"\\\n",
    "                    \"that you don't know. Use three sentences maximum and \"\\\n",
    "                    \"keep the answer concise.\\nQuestion: {question} \"\\\n",
    "                    \"\\nContext: {context} \\nAnswer:\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare Q&A chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=qdrant.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": 4,\n",
    "            \"filter\": None,\n",
    "        }\n",
    "    ), \n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Vision transformers can be applied to various real-world problems related to image and video analysis, including \n",
       "face matching, print media monitoring, industrial quality control, music scanning, strategy game playing, and \n",
       "automated machine learning.\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Vision transformers can be applied to various real-world problems related to image and video analysis, including \n",
       "face matching, print media monitoring, industrial quality control, music scanning, strategy game playing, and \n",
       "automated machine learning.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run Q&A chain\n",
    "question = \"Give examples of how Vision Transformers can be applied to real-life problems.\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# display the answer\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Give examples of how Vision Transformers can be applied to real-life problems.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'result'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">' Vision transformers can be applied to various real-world problems related to image and video </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">analysis, including face matching, print media monitoring, industrial quality control, music scanning, strategy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">game playing, and automated machine learning.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'source_documents'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Humans are generally good at learning abstract concepts about objects and\\nscenes (e.g.\\\\</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">spatial orientation, relative sizes, etc.). Over the last years\\nconvolutional neural networks have achieved almost</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">human performance in\\nrecognizing concrete classes (i.e.\\\\ specific object categories). This paper\\ntests the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance of a current CNN (GoogLeNet) on the task of\\ndifferentiating between abstract classes which are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trivially differentiable for\\nhumans. We trained and tested the CNN on the two abstract classes of horizontal\\nand </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vertical orientation and determined how well the network is able to\\ntransfer the learned classes to other, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">previously unseen objects.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'publish_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2016-06-17T12:51:23Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Learning Abstract Classes using Deep Learning'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Deep learning with neural networks is applied by an increasing number of\\npeople outside </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of classic research environments, due to the vast success of the\\nmethodology on a wide range of machine perception</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks. While this interest is\\nfueled by beautiful success stories, practical work in deep learning on novel\\ntasks</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without existing baselines remains challenging. This paper explores the\\nspecific challenges arising in the realm </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of real world tasks, based on case\\nstudies from research \\\\&amp; development in conjunction with industry, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracts\\nlessons learned from them. It thus fills a gap between the publication of\\nlatest algorithmic and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methodical developments, and the usually omitted\\nnitty-gritty of how to make them work. Specifically, we give </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">insight into deep\\nlearning projects on face matching, print media monitoring, industrial quality\\ncontrol, music </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scanning, strategy game playing, and automated machine learning,\\nthereby providing best practices for deep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learning in practice.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'publish_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2018-07-13T07:22:45Z'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Deep Learning in the Wild'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"This article investigates deep learning methodologies for single-modality\\nclinical data </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">analysis, as a crucial precursor to multi-modal medical research.\\nBuilding on Guo JingYuan's work, the study </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">refines clinical data processing\\nthrough Compact Convolutional Transformer (CCT), Patch Up, and the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">innovative\\nCamCenterLoss technique, establishing a foundation for future multimodal\\ninvestigations. The proposed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methodology demonstrates improved prediction\\naccuracy and at tentiveness to critically ill patients compared to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Guo\\nJingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained\\nvision transformer backbone to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">perform transfer learning time-series clinical\\ndata.The study highlights the potential of CCT, Patch Up, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">novel\\nCamCenterLoss in processing single modality clinical data within deep learning\\nframeworks, paving the way </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for future multimodal medical research and promoting\\nprecision and personalized healthcare\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'publish_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-03-06T00:36:05Z'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Multi-modal Deep Learning'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'This book is the result of a seminar in which we reviewed multimodal\\napproaches and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attempted to create a solid overview of the field, starting with\\nthe current state-of-the-art approaches in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two subfields of Deep Learning\\nindividually. Further, modeling frameworks are discussed where one modality </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is\\ntransformed into the other, as well as models in which one modality is utilized\\nto enhance representation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learning for the other. To conclude the second part,\\narchitectures with a focus on handling both modalities </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">simultaneously are\\nintroduced. Finally, we also cover other modalities as well as general-purpose\\nmulti-modal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models, which are able to handle different tasks on different\\nmodalities within one unified architecture. One </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interesting application\\n(Generative Art) eventually caps off this booklet.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'publish_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-01-12T07:42:36Z'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Multimodal Deep Learning'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'query'\u001b[0m: \u001b[32m'Give examples of how Vision Transformers can be applied to real-life problems.'\u001b[0m,\n",
       "    \u001b[32m'result'\u001b[0m: \u001b[32m' Vision transformers can be applied to various real-world problems related to image and video \u001b[0m\n",
       "\u001b[32manalysis, including face matching, print media monitoring, industrial quality control, music scanning, strategy \u001b[0m\n",
       "\u001b[32mgame playing, and automated machine learning.'\u001b[0m,\n",
       "    \u001b[32m'source_documents'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Humans are generally good at learning abstract concepts about objects and\\nscenes \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g.\\\\\u001b[0m\n",
       "\u001b[32mspatial orientation, relative sizes, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Over the last years\\nconvolutional neural networks have achieved almost\u001b[0m\n",
       "\u001b[32mhuman performance in\\nrecognizing concrete classes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e.\\\\ specific object categories\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This paper\\ntests the \u001b[0m\n",
       "\u001b[32mperformance of a current CNN \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGoogLeNet\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on the task of\\ndifferentiating between abstract classes which are \u001b[0m\n",
       "\u001b[32mtrivially differentiable for\\nhumans. We trained and tested the CNN on the two abstract classes of horizontal\\nand \u001b[0m\n",
       "\u001b[32mvertical orientation and determined how well the network is able to\\ntransfer the learned classes to other, \u001b[0m\n",
       "\u001b[32mpreviously unseen objects.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'publish_date'\u001b[0m: \u001b[32m'2016-06-17T12:51:23Z'\u001b[0m,\n",
       "                \u001b[32m'title'\u001b[0m: \u001b[32m'Learning Abstract Classes using Deep Learning'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Deep learning with neural networks is applied by an increasing number of\\npeople outside \u001b[0m\n",
       "\u001b[32mof classic research environments, due to the vast success of the\\nmethodology on a wide range of machine perception\u001b[0m\n",
       "\u001b[32mtasks. While this interest is\\nfueled by beautiful success stories, practical work in deep learning on novel\\ntasks\u001b[0m\n",
       "\u001b[32mwithout existing baselines remains challenging. This paper explores the\\nspecific challenges arising in the realm \u001b[0m\n",
       "\u001b[32mof real world tasks, based on case\\nstudies from research \\\\& development in conjunction with industry, and \u001b[0m\n",
       "\u001b[32mextracts\\nlessons learned from them. It thus fills a gap between the publication of\\nlatest algorithmic and \u001b[0m\n",
       "\u001b[32mmethodical developments, and the usually omitted\\nnitty-gritty of how to make them work. Specifically, we give \u001b[0m\n",
       "\u001b[32minsight into deep\\nlearning projects on face matching, print media monitoring, industrial quality\\ncontrol, music \u001b[0m\n",
       "\u001b[32mscanning, strategy game playing, and automated machine learning,\\nthereby providing best practices for deep \u001b[0m\n",
       "\u001b[32mlearning in practice.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'publish_date'\u001b[0m: \u001b[32m'2018-07-13T07:22:45Z'\u001b[0m, \u001b[32m'title'\u001b[0m: \u001b[32m'Deep Learning in the Wild'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"This\u001b[0m\u001b[32m article investigates deep learning methodologies for single-modality\\nclinical data \u001b[0m\n",
       "\u001b[32manalysis, as a crucial precursor to multi-modal medical research.\\nBuilding on Guo JingYuan's work, the study \u001b[0m\n",
       "\u001b[32mrefines clinical data processing\\nthrough Compact Convolutional Transformer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCCT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Patch Up, and the \u001b[0m\n",
       "\u001b[32minnovative\\nCamCenterLoss technique, establishing a foundation for future multimodal\\ninvestigations. The proposed \u001b[0m\n",
       "\u001b[32mmethodology demonstrates improved prediction\\naccuracy and at tentiveness to critically ill patients compared to \u001b[0m\n",
       "\u001b[32mGuo\\nJingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained\\nvision transformer backbone to\u001b[0m\n",
       "\u001b[32mperform transfer learning time-series clinical\\ndata.The study highlights the potential of CCT, Patch Up, and \u001b[0m\n",
       "\u001b[32mnovel\\nCamCenterLoss in processing single modality clinical data within deep learning\\nframeworks, paving the way \u001b[0m\n",
       "\u001b[32mfor future multimodal medical research and promoting\\nprecision and personalized healthcare\"\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'publish_date'\u001b[0m: \u001b[32m'2024-03-06T00:36:05Z'\u001b[0m, \u001b[32m'title'\u001b[0m: \u001b[32m'Multi-modal Deep Learning'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'This book is the result of a seminar in which we reviewed multimodal\\napproaches and \u001b[0m\n",
       "\u001b[32mattempted to create a solid overview of the field, starting with\\nthe current state-of-the-art approaches in the \u001b[0m\n",
       "\u001b[32mtwo subfields of Deep Learning\\nindividually. Further, modeling frameworks are discussed where one modality \u001b[0m\n",
       "\u001b[32mis\\ntransformed into the other, as well as models in which one modality is utilized\\nto enhance representation \u001b[0m\n",
       "\u001b[32mlearning for the other. To conclude the second part,\\narchitectures with a focus on handling both modalities \u001b[0m\n",
       "\u001b[32msimultaneously are\\nintroduced. Finally, we also cover other modalities as well as general-purpose\\nmulti-modal \u001b[0m\n",
       "\u001b[32mmodels, which are able to handle different tasks on different\\nmodalities within one unified architecture. One \u001b[0m\n",
       "\u001b[32minteresting application\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mGenerative Art\u001b[0m\u001b[32m)\u001b[0m\u001b[32m eventually caps off this booklet.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'publish_date'\u001b[0m: \u001b[32m'2023-01-12T07:42:36Z'\u001b[0m, \u001b[32m'title'\u001b[0m: \u001b[32m'Multimodal Deep Learning'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
