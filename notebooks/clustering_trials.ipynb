{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, json, random, sys\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from typing import Any, Dict, List, Literal, Optional, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import umap\n",
    "import yaml\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from loguru import logger\n",
    "from rich import print\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "from sklearn.metrics import (\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "from transformers import AutoTokenizer, BertModel, BertTokenizer, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU availability\n",
    "sys.path.append(\"../\")\n",
    "from src.ai_news_digest.utils import check_gpu_availability, create_run_folder\n",
    "from src.ai_news_digest.steps.benchmark import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = check_gpu_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG_PATH = \"../conf/base/cluster_bench_models.yml\"\n",
    "DATA_PATH = \"../data/03_primary/arxiv_dict_2023-11-06_00-22-42.json\"\n",
    "\n",
    "MODEL_KWARGS = {\"device\": device}\n",
    "ENCODE_KWARGS = {\n",
    "    \"normalize_embeddings\": True,\n",
    "    \"batch_size\": 16,\n",
    "    \"output_value\": \"sentence_embedding\",\n",
    "    \"convert_to_numpy\": True,\n",
    "    \"show_progress_bar\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INFO_DICT = \"data/03_primary/arxiv_dict_2023-11-06_00-22-42.json\"\n",
    "MODEL_ID = \"BAAI/bge-small-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    info_dict = json.load(f)[\"results\"]\n",
    "logger.info(f\"Successfully loaded prepared data from : {DATA_PATH}\")\n",
    "\n",
    "# retrieve abstracts, titles, dates & paper IDs\n",
    "df_data = pd.DataFrame(info_dict).transpose()\n",
    "\n",
    "# display\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_ID,\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs=ENCODE_KWARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings\n",
    "embeddings = np.array(hf.embed_documents(df_data[\"abstract\"]))\n",
    "\n",
    "# store embeddings in a dataframe\n",
    "df_embed = pd.DataFrame(\n",
    "    data=embeddings, \n",
    "    columns=[f\"embed_{i}\" for i in range(embeddings.shape[1])],\n",
    "    index=df_data.index\n",
    ")\n",
    "\n",
    "# display\n",
    "df_embed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set umap params\n",
    "umap_kwargs = {\n",
    "    \"n_neighbors\": 5,\n",
    "    \"min_dist\": 0.001,\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": \"cosine\",\n",
    "}\n",
    "\n",
    "# instanciate umap projector\n",
    "reducer = umap.UMAP(random_state=123, **umap_kwargs)\n",
    "\n",
    "# project data\n",
    "umap_proj = reducer.fit_transform(df_embed)\n",
    "\n",
    "# normalize umap coords\n",
    "umap_proj = (umap_proj - umap_proj.min(axis=0)) / (umap_proj.max(axis=0) - umap_proj.min(axis=0))\n",
    "\n",
    "# store in a dataframe with metadata\n",
    "df_umap = pd.DataFrame(columns=[f\"umap_{i}\" for i in range(umap_proj.shape[1])], data=umap_proj)\n",
    "df_umap = pd.concat((df_umap, df_data.reset_index(names=[\"ID\"])), axis=1)\n",
    "\n",
    "# display\n",
    "df_umap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster = df_umap[[col for col in df_umap.columns if \"umap\" in col]]\n",
    "\n",
    "clustering = HDBSCAN(\n",
    "    min_cluster_size=10, \n",
    "    min_samples=3, \n",
    "    max_cluster_size=None, \n",
    "    cluster_selection_epsilon=0.05\n",
    ")\n",
    "clustering.fit(X_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz\n",
    "df_umap[\"cluster\"] = [str(elt) for elt in clustering.labels_]\n",
    "df_umap[\"noise\"] = [int(elt==-1) for elt in clustering.labels_]\n",
    "fig = px.scatter(\n",
    "    df_umap,\n",
    "    x=\"umap_0\",\n",
    "    y=\"umap_1\",\n",
    "    hover_data=[\n",
    "        \"title\",\n",
    "        \"ID\",\n",
    "    ],\n",
    "    color=\"cluster\",\n",
    "    symbol=\"noise\",\n",
    "    # color_continuous_scale=px.colors.qualitative.D3,\n",
    "    category_orders={\"cluster\": list(np.sort(pd.unique(clustering.labels_)).astype(str))},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy ↓\n",
    "entropy(X_cluster.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette ↑\n",
    "silhouette_score(X_cluster, df_umap[\"cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refacto projection & clustering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_pipeline(\n",
    "    df_embed: pd.DataFrame,\n",
    "    umap_kwargs: dict,\n",
    "    clustering_kwargs: Optional[dict]=None,\n",
    "    random_state: int=123,\n",
    "    df_data: Optional[pd.DataFrame]=None,\n",
    ") -> Tuple[pd.DataFrame, Any]:\n",
    "    \n",
    "    #--- dimensionality reduction ---\n",
    "\n",
    "    # instanciate umap projector\n",
    "    reducer = umap.UMAP(random_state=random_state, **umap_kwargs)\n",
    "\n",
    "    # project data\n",
    "    umap_proj = reducer.fit_transform(df_embed)\n",
    "\n",
    "    # normalize umap coords\n",
    "    umap_proj = (umap_proj - umap_proj.min(axis=0)) / (umap_proj.max(axis=0) - umap_proj.min(axis=0))\n",
    "\n",
    "    # store in a dataframe\n",
    "    df_umap = pd.DataFrame(columns=[f\"umap_{i}\" for i in range(umap_proj.shape[1])], data=umap_proj)\n",
    "\n",
    "    # add metadata if available\n",
    "    if df_data is not None:\n",
    "        df_umap = pd.concat((df_umap, df_data.reset_index(names=[\"ID\"])), axis=1)\n",
    "\n",
    "    #--- clustering ---\n",
    "    if clustering_kwargs is not None:\n",
    "        X_cluster = df_umap[[col for col in df_umap.columns if \"umap\" in col]]\n",
    "        clustering = HDBSCAN(**clustering_kwargs)\n",
    "        clustering.fit(X_cluster)\n",
    "        df_umap[\"cluster\"] = [str(elt) for elt in clustering.labels_]\n",
    "        df_umap[\"noise\"] = [int(elt==-1) for elt in clustering.labels_]\n",
    "    \n",
    "    else:\n",
    "        clustering = None\n",
    "\n",
    "    #--- result ---\n",
    "    return df_umap, clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_kwargs = {    \n",
    "    \"min_cluster_size\": 10, \n",
    "    \"min_samples\": 3, \n",
    "    \"max_cluster_size\": None, \n",
    "    \"cluster_selection_epsilon\": 0.05,\n",
    "}\n",
    "\n",
    "umap_kwargs = {\n",
    "    \"n_neighbors\": 5,\n",
    "    \"min_dist\": 0.1,\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": \"cosine\",\n",
    "}\n",
    "\n",
    "df_umap, clustering = clustering_pipeline(\n",
    "    df_embed,\n",
    "    umap_kwargs,\n",
    "    clustering_kwargs,\n",
    "    random_state=123,\n",
    "    df_data=df_data,\n",
    ")\n",
    "\n",
    "X_cluster = df_umap[[c for c in df_umap.columns if \"umap_\" in c]]\n",
    "\n",
    "# Silhouette ↑\n",
    "print(f\" Silhouette ↑ = {silhouette_score(X_cluster, df_umap['cluster'])}\")\n",
    "\n",
    "# Entropy ↓\n",
    "print(f\" Entropy ↓ = {entropy(X_cluster.values)}\")\n",
    "\n",
    "# viz\n",
    "df_umap[\"cluster\"] = [str(elt) for elt in clustering.labels_]\n",
    "df_umap[\"noise\"] = [int(elt==-1) for elt in clustering.labels_]\n",
    "fig = px.scatter(\n",
    "    df_umap,\n",
    "    x=\"umap_0\",\n",
    "    y=\"umap_1\",\n",
    "    hover_data=[\n",
    "        \"title\",\n",
    "        \"ID\",\n",
    "    ],\n",
    "    color=\"cluster\",\n",
    "    symbol=\"noise\",\n",
    "    # color_continuous_scale=px.colors.qualitative.D3,\n",
    "    category_orders={\"cluster\": list(np.sort(pd.unique(clustering.labels_)).astype(str))},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search over projection and clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "from tqdm.contrib.itertools import product\n",
    "\n",
    "clustering_grid = {\n",
    "    \"min_cluster_size\": [3, 5, 10, 15], \n",
    "    \"min_samples\": [3, 5, 10], \n",
    "    \"max_cluster_size\": [None, 15, 25, 40], \n",
    "    \"cluster_selection_epsilon\": [\n",
    "        # 0.0, \n",
    "        # 0.05, \n",
    "        # 0.0,\n",
    "        # 0.001,\n",
    "        # 0.01,\n",
    "        0.05,\n",
    "        0.1,    \n",
    "        0.5,    \n",
    "    ],\n",
    "}\n",
    "\n",
    "umap_grid = {\n",
    "    \"n_neighbors\": [2, 4, 8, 10],\n",
    "    \"min_dist\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"n_components\": [2],\n",
    "    \"metric\": [\"cosine\"],\n",
    "    \"init\": [\"random\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_entropy = np.inf\n",
    "best_umap_kwargs = None\n",
    "df_umap_search = pd.DataFrame(columns=list(umap_grid.keys())+[\"Entropy ↓\"])\n",
    "\n",
    "for umap_vals in product(*tuple(umap_grid.values())):\n",
    "    \n",
    "    # retrieve current umap kwargs\n",
    "    umap_kwargs = {}\n",
    "    for i, key in enumerate(umap_grid.keys()):\n",
    "        umap_kwargs[key] = umap_vals[i]\n",
    "    \n",
    "    try:\n",
    "        df_umap, clustering = clustering_pipeline(\n",
    "            df_embed,\n",
    "            umap_kwargs,\n",
    "            clustering_kwargs=None,\n",
    "            random_state=123,\n",
    "            df_data=df_data,\n",
    "        )\n",
    "        X_cluster = df_umap[[c for c in df_umap.columns if \"umap_\" in c]]\n",
    "        curr_entropy = entropy(X_cluster.values)\n",
    "        df_umap_search = pd.concat(\n",
    "            (\n",
    "                df_umap_search,\n",
    "                pd.DataFrame(data=np.array([umap_vals+(curr_entropy,)]), columns=df_umap_search.columns)\n",
    "            ),\n",
    "            axis=0,\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        if curr_entropy < best_entropy:\n",
    "            best_entropy = curr_entropy\n",
    "            best_umap_kwargs = umap_kwargs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" {umap_kwargs} failed with following error: {e}; will proceed to next iteration\")\n",
    "    \n",
    "print(f\"Best entropy: {best_entropy}\")\n",
    "print(f\"Best umap_kwargs: {best_umap_kwargs}\")\n",
    "\n",
    "df_umap_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = -np.inf\n",
    "best_clustering_kwargs = None\n",
    "df_clustering_search = pd.DataFrame(columns=list(clustering_grid.keys())+[\"Silhouette ↑\"])\n",
    "\n",
    "for clustering_vals in product(*tuple(clustering_grid.values())):\n",
    "    \n",
    "    # retrieve current clustering kwargs\n",
    "    clustering_kwargs = {}\n",
    "    for i, key in enumerate(clustering_grid.keys()):\n",
    "        clustering_kwargs[key] = clustering_vals[i]\n",
    "    \n",
    "    try:\n",
    "        df_umap, clustering = clustering_pipeline(\n",
    "            df_embed,\n",
    "            best_umap_kwargs,\n",
    "            clustering_kwargs=clustering_kwargs,\n",
    "            random_state=123,\n",
    "            df_data=df_data,\n",
    "        )\n",
    "        X_cluster = df_umap[[c for c in df_umap.columns if \"umap_\" in c]]\n",
    "        curr_silhouette = silhouette_score(X_cluster.values, df_umap[\"cluster\"])\n",
    "        df_clustering_search = pd.concat(\n",
    "            (\n",
    "                df_clustering_search,\n",
    "                pd.DataFrame(data=np.array([clustering_vals+(curr_silhouette,)]), columns=df_clustering_search.columns)\n",
    "            ),\n",
    "            axis=0,\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        if curr_silhouette > best_silhouette:\n",
    "            best_silhouette = curr_silhouette\n",
    "            best_clustering_kwargs = clustering_kwargs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" {umap_kwargs} failed with following error: {e}; will proceed to next iteration\")\n",
    "    \n",
    "print(f\"Best silhouette: {best_silhouette}\")\n",
    "print(f\"Best clustering_kwargs: {best_clustering_kwargs}\")\n",
    "\n",
    "df_clustering_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_search[\"Silhouette ↑\"].sort_values().reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_umap, clustering = clustering_pipeline(\n",
    "    df_embed,\n",
    "    best_umap_kwargs,\n",
    "    best_clustering_kwargs,\n",
    "    random_state=123,\n",
    "    df_data=df_data,\n",
    ")\n",
    "\n",
    "X_cluster = df_umap[[c for c in df_umap.columns if \"umap_\" in c]]\n",
    "\n",
    "# Silhouette ↑\n",
    "print(f\" Silhouette ↑ = {silhouette_score(X_cluster, df_umap['cluster'])}\")\n",
    "\n",
    "# Entropy ↓\n",
    "print(f\" Entropy ↓ = {entropy(X_cluster.values)}\")\n",
    "\n",
    "# viz\n",
    "df_umap[\"cluster\"] = [str(elt) for elt in clustering.labels_]\n",
    "df_umap[\"noise\"] = [int(elt==-1) for elt in clustering.labels_]\n",
    "fig = px.scatter(\n",
    "    df_umap,\n",
    "    x=\"umap_0\",\n",
    "    y=\"umap_1\",\n",
    "    hover_data=[\n",
    "        \"title\",\n",
    "        \"ID\",\n",
    "    ],\n",
    "    color=\"cluster\",\n",
    "    symbol=\"noise\",\n",
    "    # color_continuous_scale=px.colors.qualitative.D3,\n",
    "    category_orders={\"cluster\": list(np.sort(pd.unique(clustering.labels_)).astype(str))},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
